training:
 batch_size: 64
 lr: 1.0E-06
 use_gpu: False
 use_pretrained: False
 test: False
 pretrained_path: ".neptune/lseq/BIG-300/checkpoints/latest.ckpt"
 max_epochs: 1
 early_stop_patience: 100000
 devices: 1
 num_nodes: 1
 
neptune:
 log: False
 api_key: "eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiI5YzViZjJlYy00NDNhLTRhN2EtOGZmYy00NDEzODBmNTgxYzMifQ=="
 project: "NTLAB/bigsleep"

model_parameters:
 lseq:
  batch_size: 8
  features: 129
  epochs: 200
  sequences: 29
  classes: 5
  lr: 2.0E-03
  seed: 4
  weight_decay: 0.001
  F: 129
  M: 32
  num_channels: 2
  minF: 0
  maxF: 50
  source_samplerate: 128
  samplerate: 100
  K: 10
  B: 20
  lstm_hidden_size: 64
  fc_hidden_size: 512
  classes: 5
  attention_size: 64

 usleep:
  batch_size: 64
  epochs: 35